name: Run Crawler

on:
  schedule:
    - cron: "5 0,6,12,18 * * *" # 매일 09:05, 15:05, 21:05에 실행 (UTC 기준으로 조정)
  workflow_dispatch: # 수동 실행 가능

jobs:
  run-crawler:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9" # Python 버전을 3.9로 설정

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip xvfb libxi6 libgconf-2-4
          # 안정적인 버전의 Chrome 설치 (예: 버전 117)
          wget https://dl.google.com/linux/chrome/deb/pool/main/g/google-chrome-stable/google-chrome-stable_117.0.5938.62-1_amd64.deb -O /tmp/chrome.deb
          sudo apt install -y /tmp/chrome.deb

          # Python 패키지 설치
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run crawler
        env:
          PAT: ${{ secrets.PAT }} # Personal Access Token을 환경 변수로 설정
        run: |
          git config --global user.name "Your Name"  # Git 사용자 이름 설정
          git config --global user.email "you@example.com"  # Git 사용자 이메일 설정
          python crawler.py  # 크롤러 실행

      - name: Commit and push changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # GitHub 토큰
        run: |
          git add data.json crawled_data.db  # 변경된 파일 추가
          git commit -m "Update data.json and crawled_data.db with crawled data" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.PAT }}@github.com/nhhanaro/nhhanaro.git HEAD:master  # 변경사항 푸시

      - name: Upload data.json
        uses: actions/upload-artifact@v3
        with:
          name: data-json
          path: data.json

      - name: Upload crawled_data.db
        uses: actions/upload-artifact@v3
        with:
          name: crawled-data-db
          path: crawled_data.db
